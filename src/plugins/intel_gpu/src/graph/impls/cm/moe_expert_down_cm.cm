// Copyright (C) 2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

namespace KERNEL_NAME {

#define GEN_KERNEL

#undef CM_DEBUG

#include <cm/cm.h>

#ifdef CMRT_EMU
# include <shim_support.h>
#endif  //  CMRT_EMU

typedef half fp16;

#ifndef BTI
#define BUFFERINDEX(x) int * x [[type("svmptr_t")]]
#define LSC_LOAD(DATA_TYPE, VS, L1, L3) cm_ptr_load<DATA_TYPE,VS,DataSize::Default,CacheHint::L1,CacheHint::L3>
#define LSC_STORE(DATA_TYPE, VS, L1, L3) cm_ptr_store<DATA_TYPE,VS,DataSize::Default,CacheHint::L1,CacheHint::L3>
#define LSC_LOAD_GATHER(DATA_TYPE, VS, L1, L3,channels) cm_ptr_load<DATA_TYPE,VS,DataSize::Default,CacheHint::L1,CacheHint::L3, channels>
//#define LSC_STORE_2D(DATA_TYPE, h, w, L1, L3) cm_store<DATA_TYPE,h, w, CacheHint::L1,CacheHint::L3>
#define LSC_STORE_2D(DATA_TYPE, h, w, L1, L3) cm_ptr_store<DATA_TYPE,h, w, CacheHint::L1,CacheHint::L3>
#define LSC_LOAD_0(DATA_TYPE, VS)  cm_ptr_load<DATA_TYPE,VS,DataSize::Default,CacheHint::Cached,CacheHint::Cached>
#else
#define BUFFERINDEX(x) SurfaceIndex x [[type("buffer_t")]]
#define LSC_LOAD(DATA_TYPE, VS, L1, L3) cm_load<DATA_TYPE,VS,DataSize::Default,CacheHint::L1,CacheHint::L3>
#define LSC_STORE(DATA_TYPE, VS, L1, L3) cm_store<DATA_TYPE,VS,DataSize::Default,CacheHint::L1,CacheHint::L3>
#define LSC_LOAD_GATHER(DATA_TYPE, VS, L1, L3,channels) cm_load<DATA_TYPE,VS,DataSize::Default,CacheHint::L1,CacheHint::L3, channels>
#define LSC_STORE_2D(DATA_TYPE, h, w, L1, L3) cm_store<DATA_TYPE,h, w, CacheHint::L1,CacheHint::L3>
#endif


constexpr int QK = 128; // compatible
constexpr int SBS = 2; // compatible
constexpr int BLOCK_SIZE = QK / 2;
constexpr int SCALE_SIZE = sizeof(fp16);
#define NUM_EXPERTS             8

inline auto load_qblocks(const uint8_t* weight, const uint8_t* scale, const uint8_t* zero_point, uint32_t zp_offset)
{
    vector<uint8_t, BLOCK_SIZE* SBS> ybytes;   //128bytes
    ybytes.format<int>() = LSC_LOAD_0(int, (BLOCK_SIZE * SBS / 4))((int*)weight, 0);

    vector<uint8_t, 4> zero_points;              //TODO size: SBS/2 = 1
    zero_points.format<int>() = LSC_LOAD_0(int, 1)((int*)zero_point, zp_offset - (zp_offset % 4));
    zero_points[0] = zero_points[zp_offset % 4];

    vector<fp16, SBS> scales; 
    scales.format<int>() = LSC_LOAD_0(int, (SBS / 2))((int*)scale, 0); //4 bytes 2x scale
    //vector<fp16, SBS> scales = LSC_LOAD(fp16, SBS, Cached, Cached)((fp16*)scale, 0);  compiler error


    vector<fp16, QK* SBS> yvs;
#pragma unroll
    for (int i = 0; i < SBS; ++i) {
        int8_t zp = ((i % 2 == 0) ? (int8_t)(zero_points[i / 2] & (uint8_t)0xF) : (int8_t)(zero_points[i / 2] >> (uint8_t)4));
        vector<uint8_t, QK> uyv;
        // uyv.select<QK / 2, 1>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
        // uyv.select<QK / 2, 1>(QK / 2) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
        uyv.select<QK / 2, 2>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF; // compatible
        uyv.select<QK / 2, 2>(1) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4; // compatible
        yvs.template select<QK, 1>(i * QK) = (uyv.format<int8_t>() - (int8_t)zp) * scales[i];
    }
    return yvs;
}

// C++ doesn't support function template partial specialization, so write a new version for SBS=1
inline auto load_qblock(const uint8_t* weight, const uint8_t* scale, const uint8_t* zero_point, bool is_even) {
    vector<uint8_t, BLOCK_SIZE> ybytes;
    ybytes.format<int>() = LSC_LOAD_0(int, (BLOCK_SIZE / 4))((int*)weight, 0); //read 64 bytes
    fp16 scales = *(const fp16*)scale;      // 1x scale   TODO pointer access
    uint8_t zero_points = *(const uint8_t*)zero_point;

    int8_t zp = ((is_even) ? (int8_t)(zero_points & (uint8_t)0xF) : (int8_t)(zero_points >> (uint8_t)4));
    vector<uint8_t, QK> uyv;
    // uyv.select<QK / 2, 1>(0) = ybytes & (uint8_t)0xF;
    // uyv.select<QK / 2, 1>(QK / 2) = ybytes >> (uint8_t)4;
    uyv.select<QK / 2, 2>(0) = ybytes & (uint8_t)0xF; // compatible
    uyv.select<QK / 2, 2>(1) = ybytes >> (uint8_t)4; // compatible
    vector<fp16, QK> yv = (uyv.format<int8_t>() - (int8_t)8) * scales;

    return yv;
}

template <typename IT, const int VS, const int GS, const int ES>
void moe_forward_down_kernel(
    BUFFERINDEX(input),
    BUFFERINDEX(indexs),
    BUFFERINDEX(eweights),
    BUFFERINDEX(base_addrs),
    BUFFERINDEX(down_addrs),
    BUFFERINDEX(down_scales_addrs),
    BUFFERINDEX(down_zp_addrs),
    BUFFERINDEX(output),
    const int num_experts,
    const int state_size,
    const int output_size,
    uint slmX
) {
    //cm_slm_init(GS * VS * sizeof(float));
    //uint slmX = cm_slm_alloc(GS * VS * sizeof(float));

    const int nb = state_size / QK;
    const int nsb = nb / SBS;

    const int eid = cm_group_id(0) * cm_local_size(0) + cm_local_id(0);
    const int tid = cm_local_id(1);
    const int vid = cm_group_id(1) * VS;

    //vector<int, NUM_EXPERTS * 2> indexs_vec = LSC_LOAD_0(int, NUM_EXPERTS * 2)(indexs, 0);
    //int index = indexs_vec.format<unsigned long long>()[eid];

    //32bit index
    vector<int, NUM_EXPERTS> indexs_vec = LSC_LOAD_0(int, NUM_EXPERTS)(indexs, 0);
    int index = indexs_vec[eid];

    vector<IT, NUM_EXPERTS> eweights_vec;
    eweights_vec.format<int>() = LSC_LOAD_0(int, 4)(eweights, 0);   //8 fp16
    const IT eweight = eweights_vec[eid];

    int g_offset = index / 8;
    int l_offset = index % 8;
    vector<int, 16> gpu_down_addr = LSC_LOAD_0(int, 16)(down_addrs, g_offset * 64);
    const uint8_t* weight = (uint8_t*)base_addrs + gpu_down_addr.format<unsigned long long>()[l_offset];

    vector<int, 16> gpu_down_scales_addr = LSC_LOAD_0(int, 16)(down_scales_addrs, g_offset * 64);
    const uint8_t* scales = (const uint8_t*)base_addrs + gpu_down_scales_addr.format<unsigned long long>()[l_offset];
  
    vector<int, 16> gpu_down_zp_addr = LSC_LOAD_0(int, 16)(down_zp_addrs, g_offset * 64);
    const uint8_t* zp1 = (const uint8_t*)base_addrs + gpu_down_zp_addr.format<unsigned long long>()[l_offset];


    int input_base_offset = eid * state_size * sizeof(IT);

    const uint8_t* weight_base = weight + nb * BLOCK_SIZE * vid;
    const uint8_t* scale_base = scales + nb * SCALE_SIZE * vid;
    const uint8_t* zp1_base = zp1 + nb / 2 * vid;

    vector<IT, VS* ES> accvs{};

    for (int s = tid; s < nsb; s += GS) {
        vector<IT, SBS* QK> xvs;      //512 bytes
        xvs.format<int>().select<64, 1>(0) = LSC_LOAD_0(int, (SBS * QK / 4))((int*)input, input_base_offset + s * SBS * QK * sizeof(IT));
        xvs.format<int>().select<64, 1>(64) = LSC_LOAD_0(int, (SBS * QK / 4))((int*)input, input_base_offset + s * SBS * QK * sizeof(IT) + 256);

#pragma unroll
        for (int v = 0; v < VS; ++v) {
            vector<fp16, SBS* QK> yvs = load_qblocks(
                weight_base + v * nb * BLOCK_SIZE + s * SBS * BLOCK_SIZE,
                scale_base + v * nb * SCALE_SIZE + s * SBS * SCALE_SIZE,
                zp1_base, v * nb / 2 + s * SBS / 2
            );

#pragma unroll
            for (int i = 0; i < SBS * QK; i += ES) {
                accvs.template select<ES, 1>(v * ES) +=
                    xvs.template select<ES, 1>(i) *
                    yvs.template select<ES, 1>(i);
            }
        }
    }

    for (int b = nsb * SBS + tid; b < nb; b += GS) {
        vector<IT, QK> xv;   //256bytes
        xv.format<int>() = LSC_LOAD_0(int, (QK / 2))((int*)input, input_base_offset + b * QK * sizeof(IT));

#pragma unroll
        for (int v = 0; v < VS; ++v) {
            vector<fp16, QK> yv = load_qblock(
                weight_base + v * nb * BLOCK_SIZE + b * BLOCK_SIZE,
                scale_base + v * nb * SCALE_SIZE + b * SCALE_SIZE,
                zp1_base + v * nb / 2 + b / 2,(b % 2 == 0));

#pragma unroll
            for (int i = 0; i < QK; i += ES) {
                accvs.template select<ES, 1>(v * ES) +=
                    xv.template select<ES, 1>(i) *
                    yv.template select<ES, 1>(i);
            }
        }
    }

    vector<float, VS> accs;
#pragma unroll
    for (int v = 0; v < VS; ++v) {
        accs[v] = cm_sum<float>(accvs.template select<ES, 1>(v * ES));
    }

    cm_slm_block_write<float, VS >(slmX, tid * VS * sizeof(float), accs);

    cm_slm_fence(0x20);
    cm_barrier();

    if (tid == 0) {
#pragma unroll
        for (int i = 1; i < GS; ++i) {
            vector<float, VS> accs_slm;
            cm_slm_block_read<float, VS>(slmX, i * VS * sizeof(float), accs_slm);
            accs += accs_slm;
        }

        vector<IT, VS> accs_fp16 = accs * eweight;
        LSC_STORE(int, VS/2, WriteBack, WriteBack)((int*)output, (eid * output_size + vid) * sizeof(IT), accs_fp16.format<int>());//fp16
    }
}

_GENX_MAIN_ void KERNEL_NAME(
    BUFFERINDEX(input),             // shape: f16[8, 1, 768]
    BUFFERINDEX(indexs),            // shape: i32[8]
    BUFFERINDEX(eweights),          // shape: f16[8]
    BUFFERINDEX(base_addrs),        // shape: u8[?]
    BUFFERINDEX(down_addrs),        // shape: u64[128]
    BUFFERINDEX(down_scales_addrs), // shape: u64[128]
    BUFFERINDEX(down_zp_addrs),
    BUFFERINDEX(output)) {          // shape: f16[8, 2048]
    const int VS = 4;
    const int GS = 4;
    cm_slm_init(GS * VS * sizeof(float));
    uint slmX = cm_slm_alloc(GS * VS * sizeof(float));
    moe_forward_down_kernel<fp16, 4U, 4U, 32U>(
        input,                  // shape: f16[8, 1, 768]
        indexs,                 // shape: i32[8]
        eweights,               // shape: f16[8]
        base_addrs,             // shape: u8[?]
        down_addrs,             // shape: u64[128]
        down_scales_addrs,      // shape: u64[128]
        down_zp_addrs,
        output,                 // shape: f16[8, 2048]
        EXPERT_NUM,
        INTERMEDIATE_SIZE,      // const: 768
        HIDDEN_SIZE,            // const: 2048
        slmX);
}

#ifdef CMRT_EMU
EXPORT_SIGNATURE(sgemm_kernel);
#endif  //  CMRT_EMU

}
